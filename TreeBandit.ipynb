{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TreeBandit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howakuro/TreeBandit/blob/master/TreeBandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QjeYi-gjbF-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##各種インポート"
      ]
    },
    {
      "metadata": {
        "id": "gPj1R96RaQFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vR1iats2bINn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##ツリーバンディット環境\n",
        "![ツリー構造](https://i.imgur.com/IRjxJMu.jpg)<br>\n",
        "上記のような木構造のバンディットを探索する。合計期待値が最も高いルートは状態1で0.4の腕を選択し、状態2で0.9の腕を選択することである。\n"
      ]
    },
    {
      "metadata": {
        "id": "4X98OZPWapE2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TreeBandit():\n",
        "    def __init__(self):\n",
        "        self._arm_num = 4  # 腕の数\n",
        "        self._state_num = 5  # 状態数\n",
        "        self._playcount = 0  # 腕を回した回数\n",
        "        self._arm_probability = [[0.4, 0.5, 0.6, 0.7],  # 状態1\n",
        "                                 [0.9, 0.8, 0.1, 0.2],  # 状態2\n",
        "                                 [0.7, 0.1, 0.3, 0.6],  # 状態3\n",
        "                                 [0.6, 0.4, 0.1, 0.3],  # 状態4\n",
        "                                 [0.5, 0.2, 0.3, 0.1]]  # 状態5\n",
        "    #環境のリセット\n",
        "    def reset(self):\n",
        "        self._playcount = 0\n",
        "        return 0  # 状態1に初期化\n",
        "\n",
        "    # 設定された腕の確率を取得する\n",
        "    def get_probability(self):\n",
        "        return self._arm_probability\n",
        "\n",
        "    # 選んだアームのスロットを回す\n",
        "    def step(self, state, action):\n",
        "        if random.random() <= self._arm_probability[state][action]:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "        state += (action + 1)\n",
        "        self._playcount += 1\n",
        "        done = False\n",
        "        if self._playcount == 2:\n",
        "            done = True\n",
        "            state = None\n",
        "        return state, reward, done, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSw7AlsibLZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##価値関数クラス"
      ]
    },
    {
      "metadata": {
        "id": "ThuBQP2SbOIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##期待値クラス"
      ]
    },
    {
      "metadata": {
        "id": "oSXCc3_hauXF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Expected_Value():\n",
        "    def __init__(self, state_num, action_num):\n",
        "        self.state_num = state_num #状態の数\n",
        "        self.action_num = action_num #とり得る行動の数\n",
        "       \n",
        "    def episode_reset(self):\n",
        "        pass\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.E\n",
        "    \n",
        "    def update(self, state, action, reward, next_state):\n",
        "        self.play_count[state][action] += 1\n",
        "        self.hit_count[state][action] += reward\n",
        "        self.E[state][action] = self.hit_count[state][action] / self.play_count[state][action]\n",
        "    \n",
        "    def sim_reset(self):\n",
        "        self.play_count = [[0 for j in range(self.action_num)] for i in range(self.state_num)]\n",
        "        self.hit_count = [[0 for j in range(self.action_num)] for i in range(self.state_num)]\n",
        "        self.E = [[0 for j in range(self.action_num)] for i in range(self.state_num)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4e8YZuz6bQ1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Q関数クラス"
      ]
    },
    {
      "metadata": {
        "id": "ih1kFYwMartX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Q_Learning():\n",
        "    def __init__(self, state_num, action_num, alpha =0.1,gamma = 0.99):\n",
        "        self.state_num = state_num #状態の数\n",
        "        self.action_num = action_num #とり得る行動の数\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "       \n",
        "    def episode_reset(self):\n",
        "        pass\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.Q\n",
        "    \n",
        "    def update(self, state, action, reward, next_state):\n",
        "        self.play_count[state][action] += 1\n",
        "        self.hit_count[state][action] += reward\n",
        "        if next_state == None:\n",
        "            next_Q = 0\n",
        "        else:\n",
        "            next_Q = max(self.Q[next_state])\n",
        "        td_error = reward + (self.gamma * next_Q - self.Q[state][action])\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "    \n",
        "    def sim_reset(self):\n",
        "        self.play_count = [[0 for j in range(self.action_num)] for i in range(self.state_num)]\n",
        "        self.hit_count = [[0 for j in range(self.action_num)] for i in range(self.state_num)]\n",
        "        self.Q = [[0 for j in range(self.action_num)] for i in range(self.state_num)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80b_fwJ8bU0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##方策関数"
      ]
    },
    {
      "metadata": {
        "id": "7sBMUOdGbXOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###ε減衰型ε-Greedyクラス"
      ]
    },
    {
      "metadata": {
        "id": "LwAKfJhnax0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decay_E_Greedy():\n",
        "    def __init__(self, epsilon, decay_step, decay_episode):\n",
        "        self.start_epsilon = epsilon\n",
        "        self.decay_step = decay_step\n",
        "        self.decay_episode = decay_episode\n",
        "\n",
        "    def sim_reset(self):\n",
        "        self.epsilon = self.start_epsilon\n",
        "\n",
        "    def episode_reset(self):\n",
        "        self.epsilon -= self.decay_episode\n",
        "\n",
        "    #アーム選択\n",
        "    def select_action(self,value_func,state):\n",
        "        self.epsilon -= self.decay_step\n",
        "        if self.epsilon <= random.random():  \n",
        "            idx = np.where(value_func[state] == np.max(value_func[state]))\n",
        "            return np.random.choice(idx[0])\n",
        "        else:\n",
        "           return np.random.choice([0, 1, 2, 3]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rmY-7zAbdgA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##エージェントクラス"
      ]
    },
    {
      "metadata": {
        "id": "1fU_hXyDa254",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, value, policy):\n",
        "        self.value = value#価値推定クラス\n",
        "        self.policy = policy#方策クラス\n",
        "\n",
        "    def select_action(self, state):\n",
        "        value = self.value.get_value()\n",
        "        return self.policy.select_action(value, state)\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        self.value.update(state, action, reward, next_state)\n",
        "\n",
        "    def sim_reset(self):\n",
        "        self.value.sim_reset()\n",
        "        self.policy.sim_reset()\n",
        "\n",
        "    def episode_reset(self):\n",
        "        self.value.episode_reset()\n",
        "        self.policy.episode_reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9_nMu3jbfyq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##シミュレーション関数"
      ]
    },
    {
      "metadata": {
        "id": "CYDGrIq3a4zF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simulation(env, agent, agent_name, sim_num, episode_num, step_num):\n",
        "    print(\"\\n【Agent_Name】\\n\",agent_name)\n",
        "    for sim in range(sim_num):\n",
        "        agent.sim_reset()\n",
        "        for episode in range(episode_num):\n",
        "            state = env.reset()\n",
        "            agent.episode_reset()\n",
        "            for step in range(step_num):\n",
        "                action = agent.select_action(state)\n",
        "                next_state, reward, done, info = env.step(state, action)\n",
        "                agent.update(state, action, reward, next_state)\n",
        "                if done:\n",
        "                    break\n",
        "                state = next_state\n",
        "    print(\"【Q_Array】\\n\",agent.value.get_value())\n",
        "    print(\"【Play_count】\\n\",agent.value.play_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWYsmPCubiUG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##メイン"
      ]
    },
    {
      "metadata": {
        "id": "MuCqMm7Aa7WQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f725edc5-802f-4990-ced0-582d5f99ede3"
      },
      "cell_type": "code",
      "source": [
        "ENV = TreeBandit()\n",
        "SIMULATION_NUM = 1\n",
        "EPISODE_NUM = 10000\n",
        "STEP_NUM = 2\n",
        "ARM_NUM = 4#アームの数\n",
        "STATE_NUM = 5#状態数\n",
        "Agent_list = [\n",
        "              Agent(Expected_Value(STATE_NUM, ARM_NUM), Decay_E_Greedy(0.1, 0.0, 0.0)),\n",
        "              Agent(Q_Learning(STATE_NUM,ARM_NUM) , Decay_E_Greedy(1.0, 0.0 , (1.0 / 2000)))\n",
        "             ]\n",
        "Agent_Name = [\n",
        "              \"Expected,Decay_E_Greedy\",\n",
        "              \"Q_Learning,Decay_E_Greedy\"\n",
        "             ]\n",
        "for agent,name in zip(Agent_list,Agent_Name):\n",
        "   simulation(ENV, agent, name, SIMULATION_NUM, EPISODE_NUM, STEP_NUM)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "【Agent_Name】\n",
            " Expected,Decay_E_Greedy\n",
            "【Q_Array】\n",
            " [[0.4065040650406504, 0.48917748917748916, 0.5970287836583101, 0.6946483542505328], [0.8741721854304636, 0.8360655737704918, 0.2, 0.0], [0.7282051282051282, 0.0, 0.3333333333333333, 0.5833333333333334], [0.6035751840168244, 0.38461538461538464, 0.08333333333333333, 0.2894736842105263], [0.5023678484576987, 0.25, 0.3119266055045872, 0.09004739336492891]]\n",
            "【Play_count】\n",
            " [[246, 231, 1077, 8446], [151, 61, 30, 4], [195, 6, 6, 24], [951, 26, 24, 76], [7813, 204, 218, 211]]\n",
            "\n",
            "【Agent_Name】\n",
            " Q_Learning,Decay_E_Greedy\n",
            "【Q_Array】\n",
            " [[1.1621912852037761, 0.8848813113536277, 0.8810154362488262, 0.8783852348744439], [0.8458403503849545, 0.5423465579229474, 0.13598598991865526, 0.24412542702876458], [0.6318868386267796, 0.049508018578181545, 0.2944766993525858, 0.40519820283099767], [0.45089553254764725, 0.3817999537161761, 0.031381059609000006, 0.23005347496038836], [0.41170759204180923, 0.1272340660266844, 0.06448682975971405, 0.040405263754163374]]\n",
            "【Play_count】\n",
            " [[8194, 560, 464, 782], [7744, 288, 75, 87], [322, 59, 57, 122], [249, 104, 60, 51], [589, 87, 58, 48]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}